{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, threading, os, glob\n",
    "from random import randint\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.streaming import StreamingQueryException\n",
    "import traceback, logging\n",
    "\n",
    "host = 'spark-test1'\n",
    "checkpoint = 'hdfs://spark-test1:9000/checkpoint/raw/transactions'\n",
    "xml_directory = 'hdfs://spark-test1:9000/in/transactions'\n",
    "\n",
    "# Set the location of the Delta Lake and Kafka packages\n",
    "kafka_package = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\"  # Replace with the correct Spark version\n",
    "\n",
    "# Initialize Spark Session for Kafka\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"write_to_kafka\") \\\n",
    "    .master(f\"spark://{host}:7077\") \\\n",
    "    .config(\"spark.jars.packages\", f\"{kafka_package}\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", checkpoint) \\\n",
    "    .config(\"spark.cores.max\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Kafka Configuration\n",
    "kafka_server = f\"{host}:9092\"\n",
    "topic_name = \"test-topic\"\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def stream(spark):\n",
    "    schema = StructType([\n",
    "        StructField(\"path\", StringType(), False),\n",
    "        StructField(\"modificationTime\", TimestampType(), False),\n",
    "        StructField(\"length\", LongType(), False),\n",
    "        StructField(\"content\", BinaryType(), True)\n",
    "    ])\n",
    "\n",
    "    # Read stream from directory\n",
    "    df = spark.readStream.format(\"binaryFile\") \\\n",
    "        .schema(schema) \\\n",
    "        .load(xml_directory)\n",
    "\n",
    "    # Write the stream to Kafka\n",
    "    return df.selectExpr(\"path as key\", \"to_json(struct(*)) AS value\").writeStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "        .option(\"topic\", topic_name) \\\n",
    "        .start()\n",
    "\n",
    "    # return df \\\n",
    "    #     .writeStream \\\n",
    "    #     .outputMode(\"append\") \\\n",
    "    #     .format(\"console\") \\\n",
    "    #     .start() \\\n",
    "    #     .awaitTermination()\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        stream(spark).awaitTermination()\n",
    "    except StreamingQueryException as e:\n",
    "        # Log the error message\n",
    "        print(f\"Streaming exception:\\n{traceback.format_exc()}\")\n",
    "        print(\"Restarting query after 10 seconds...\")\n",
    "        time.sleep(10)  # Sleep for 10 seconds before restarting the query\n",
    "    except Exception as e:\n",
    "        print(f\"Non-streaming exception:\\n{traceback.format_exc()}\")\n",
    "        print(f\"Restarting query after 10 seconds...\")        \n",
    "        time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
