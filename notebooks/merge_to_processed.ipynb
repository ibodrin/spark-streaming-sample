{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/spark/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/spark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/spark/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a402e44f-f946-480d-b7aa-81e66c09e4e1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound com.databricks#spark-xml_2.12;0.14.0 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;2.3.4 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.2.5 in central\n",
      ":: resolution report :: resolve 218ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.14.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.2.5 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;2.3.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a402e44f-f946-480d-b7aa-81e66c09e4e1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/6ms)\n",
      "23/12/10 07:10:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/10 07:10:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/12/10 07:10:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/12/10 07:10:23 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "\n",
    "delta_package = \"io.delta:delta-spark_2.12:3.0.0\"  # Replace with the correct Delta version\n",
    "xml_package = \"com.databricks:spark-xml_2.12:0.14.0\"\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"MergeToProcessed\").master('spark://spark-test1:7077') \\\n",
    "    .config(\"spark.jars.packages\", f\"{delta_package},{xml_package}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.cores.max\", \"1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "hdfs_path = \"hdfs://spark-test1:9000\"\n",
    "raw = os.path.join(hdfs_path, 'raw', 'transactions')\n",
    "processed = os.path.join(hdfs_path, 'processed', 'transactions')\n",
    "checkpoint = os.path.join(hdfs_path, 'checkpoint', 'processed', 'transactions')\n",
    "dlq = os.path.join(hdfs_path, 'dlq', 'processed', 'transactions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.column import Column, _to_java_column\n",
    "from pyspark.sql.types import _parse_datatype_json_string\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def ext_from_xml(xml_column, schema, options={}):\n",
    "    java_column = _to_java_column(xml_column.cast('string'))\n",
    "    java_schema = spark._jsparkSession.parseDataType(schema.json())\n",
    "    scala_map = spark._jvm.org.apache.spark.api.python.PythonUtils.toScalaMap(options)\n",
    "    jc = spark._jvm.com.databricks.spark.xml.functions.from_xml(\n",
    "        java_column, java_schema, scala_map)\n",
    "    return Column(jc)\n",
    "\n",
    "def ext_schema_of_xml_df(df, options={}):\n",
    "    assert len(df.columns) == 1\n",
    "\n",
    "    scala_options = spark._jvm.PythonUtils.toScalaMap(options)\n",
    "    java_xml_module = getattr(getattr(\n",
    "        spark._jvm.com.databricks.spark.xml, \"package$\"), \"MODULE$\")\n",
    "    java_schema = java_xml_module.schema_of_xml_df(df._jdf, scala_options)\n",
    "    return _parse_datatype_json_string(java_schema.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/10 07:10:28 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/12/10 07:10:28 WARN HadoopFSUtils: The directory hdfs://spark-test1:9000/raw/transactions/_raw_insert_date=2023-12-10/_raw_insert_hour=06/part-00000-46bb570b-d10b-489a-baec-29c50ce541fe.c000.snappy.parquet was not found. Was it deleted very recently?\n",
      "23/12/10 07:10:28 WARN HadoopFSUtils: The directory hdfs://spark-test1:9000/raw/transactions/_raw_insert_date=2023-12-10/_raw_insert_hour=06/part-00000-dda1102e-2a59-4fca-b5b4-4ca8bb8830b6.c000.snappy.parquet was not found. Was it deleted very recently?\n",
      "23/12/10 07:10:28 WARN HadoopFSUtils: The directory hdfs://spark-test1:9000/raw/transactions/_raw_insert_date=2023-12-10/_raw_insert_hour=06/part-00000-a43495e1-7682-4970-9ae0-d1b32ad259bd.c000.snappy.parquet was not found. Was it deleted very recently?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/10 07:11:21 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "----------------------------------------                                        \n",
      "Exception occurred during processing of request from ('127.0.0.1', 56128)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_45416/3044963969.py\", line 98, in process_batch\n",
      "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
      "  File \"/tmp/spark-38cf44e7-4494-4487-82dc-0c4b8a8a2c3a/userFiles-2b523086-88cf-4be2-b168-11ad7db677a5/io.delta_delta-spark_2.12-3.0.0.jar/delta/tables.py\", line 1022, in execute\n",
      "    self._jbuilder.execute()\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o245.execute\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o52.awaitTermination",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/spark/spark-streaming-sample/notebooks/merge_to_processed.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bspark/home/spark/spark-streaming-sample/notebooks/merge_to_processed.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEmpty batch\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bspark/home/spark/spark-streaming-sample/notebooks/merge_to_processed.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39m# Write the transformed data to the processed layer\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bspark/home/spark/spark-streaming-sample/notebooks/merge_to_processed.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m query \u001b[39m=\u001b[39m raw_df\u001b[39m.\u001b[39;49mwriteStream \\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bspark/home/spark/spark-streaming-sample/notebooks/merge_to_processed.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m     \u001b[39m.\u001b[39;49mforeachBatch(process_batch) \\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bspark/home/spark/spark-streaming-sample/notebooks/merge_to_processed.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mcheckpointLocation\u001b[39;49m\u001b[39m\"\u001b[39;49m, checkpoint) \\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bspark/home/spark/spark-streaming-sample/notebooks/merge_to_processed.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m     \u001b[39m.\u001b[39;49mstart() \\\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bspark/home/spark/spark-streaming-sample/notebooks/merge_to_processed.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m     \u001b[39m.\u001b[39;49mawaitTermination()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bspark/home/spark/spark-streaming-sample/notebooks/merge_to_processed.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39m# query = flattened_df \\\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bspark/home/spark/spark-streaming-sample/notebooks/merge_to_processed.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m \u001b[39m#     .writeStream \\\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bspark/home/spark/spark-streaming-sample/notebooks/merge_to_processed.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39m#     .outputMode(\"append\") \\\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bspark/home/spark/spark-streaming-sample/notebooks/merge_to_processed.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39m#     .format(\"console\") \\\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bspark/home/spark/spark-streaming-sample/notebooks/merge_to_processed.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39m#     .start()\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsq\u001b[39m.\u001b[39mawaitTermination(\u001b[39mint\u001b[39m(timeout \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsq\u001b[39m.\u001b[39;49mawaitTermination()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m answer[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o52.awaitTermination"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/spark/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define the schema of your raw layer\n",
    "raw_schema = StructType([\n",
    "    StructField(\"key\", StringType()),\n",
    "    StructField(\"value\", StringType()),\n",
    "    StructField(\"topic\", StringType()),\n",
    "    StructField(\"partition\", StringType()),\n",
    "    StructField(\"offset\", StringType()),\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"timestampType\", StringType()),\n",
    "    StructField(\"_raw_insert_timestamp\", TimestampType()),\n",
    "    StructField(\"_raw_insert_date\", StringType()),\n",
    "    StructField(\"_raw_insert_hour\", IntegerType())\n",
    "])\n",
    "# Read the data from the raw layer using structured streaming\n",
    "raw_df = spark.readStream.format(\"parquet\")\\\n",
    "    .option(\"path\", raw)\\\n",
    "    .schema(raw_schema) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "# Function to process each batch\n",
    "def process_batch(batch_df, batch_id):\n",
    "    if not batch_df.rdd.isEmpty():\n",
    "\n",
    "        json_schema = StructType([\n",
    "            StructField(\"path\", StringType()),\n",
    "            StructField(\"modificationTime\", StringType()),\n",
    "            StructField(\"length\", StringType()),\n",
    "            StructField(\"content\", StringType())\n",
    "        ])\n",
    "        parsed_json_df = batch_df.withColumn(\"json_data\", from_json(col(\"value\"), json_schema))\n",
    "\n",
    "        # Extract and decode the base64 content\n",
    "        decoded_df = parsed_json_df.withColumn(\"decoded_content\", unbase64(col(\"json_data.content\"))) \\\n",
    "            .withColumn(\"xml_content\", expr(\"CAST(decoded_content AS STRING)\"))\n",
    "\n",
    "        #schema_def = ext_schema_of_xml_df(decoded_df.select(\"xml_content\"))\n",
    "        #decoded_df = decoded_df.withColumn('test_debug', lit(schema_def).cast('string'))\n",
    "\n",
    "        xml_schema = StructType([\n",
    "            StructField(\n",
    "                'Transaction', \n",
    "                ArrayType(\n",
    "                    StructType([\n",
    "                        StructField('TransactionId', LongType(), True),\n",
    "                        StructField('Amount', FloatType(), True),\n",
    "                        StructField('CustomerId', LongType(), True),\n",
    "                        StructField('DateTime', TimestampType(), True),\n",
    "                        StructField('Location', StringType(), True),\n",
    "                        StructField('Result', StringType(), True)\n",
    "                    ]),\n",
    "                    True\n",
    "                ),\n",
    "                True\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        xml_df = decoded_df.withColumn(\n",
    "            \"parsed\",\n",
    "            ext_from_xml(\n",
    "                xml_column = col(\"xml_content\"),\n",
    "                schema=xml_schema,\n",
    "                options={\"mode\": \"FAILFAST\"}\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        windowSpec = Window.partitionBy(\"TransactionId\").orderBy(col(\"_raw_insert_timestamp\").desc())\n",
    "        # Flatten the DataFrame\n",
    "        flattened_df = xml_df.select(\n",
    "            explode(col(\"parsed.Transaction\")).alias(\"Transaction\"),\n",
    "            col('_raw_insert_timestamp').alias('_raw_insert_timestamp')\n",
    "        ).select(\n",
    "            col(\"Transaction.TransactionId\").alias(\"TransactionId\"),\n",
    "            col(\"Transaction.Amount\").alias(\"Amount\"),\n",
    "            col(\"Transaction.CustomerId\").alias(\"CustomerId\"),\n",
    "            col(\"Transaction.DateTime\").alias(\"TransactionDateTime\"),\n",
    "            to_date(col(\"Transaction.DateTime\")).alias(\"TransactionDate\"),\n",
    "            upper(trim(col(\"Transaction.Location\"))).alias(\"Location\"),\n",
    "            upper(trim(col(\"Transaction.Result\"))).alias(\"Result\"),\n",
    "            current_timestamp().alias(\"_processed_insert_timestamp\"),\n",
    "            col('_raw_insert_timestamp').alias('_raw_insert_timestamp')\n",
    "        ).withColumn(\"row_rank\", row_number().over(windowSpec)) \\\n",
    "            .filter(col(\"row_rank\") == 1) \\\n",
    "            .drop(\"row_rank\")\n",
    "\n",
    "        # Check for the existence of the Delta table\n",
    "        if DeltaTable.isDeltaTable(spark, processed):\n",
    "            # If the table exists, create a DeltaTable instance for it\n",
    "            delta_table = DeltaTable.forPath(spark, processed)\n",
    "            # Perform the merge operation\n",
    "            delta_table.alias(\"target\").merge(\n",
    "                flattened_df.alias(\"source\"),\n",
    "                \"target.TransactionId = source.TransactionId\"\n",
    "            ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "        else:\n",
    "            # If the Delta table does not exist, create one from the batch DataFrame\n",
    "            flattened_df.write.format(\"delta\").partitionBy(\"TransactionDate\").save(processed)\n",
    "    else:\n",
    "        print(\"Empty batch\")\n",
    "\n",
    "# Write the transformed data to the processed layer\n",
    "query = raw_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .option(\"checkpointLocation\", checkpoint) \\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n",
    "\n",
    "# query = flattened_df \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
