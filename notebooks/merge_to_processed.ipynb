{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/spark/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/spark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/spark/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2e62ddf1-96d9-4193-aab5-64ff359dcf20;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound com.databricks#spark-xml_2.12;0.14.0 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;2.3.4 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.2.5 in central\n",
      ":: resolution report :: resolve 233ms :: artifacts dl 40ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.14.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.2.5 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;2.3.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2e62ddf1-96d9-4193-aab5-64ff359dcf20\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/7ms)\n",
      "23/12/10 05:13:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "\n",
    "delta_package = \"io.delta:delta-spark_2.12:3.0.0\"  # Replace with the correct Delta version\n",
    "xml_package = \"com.databricks:spark-xml_2.12:0.14.0\"\n",
    "# Initialize Spark Session\n",
    "#spark = SparkSession.builder.appName(\"MergeToProcessed\").master('spark://spark-test1:7077') \\\n",
    "spark = SparkSession.builder.appName(\"MergeToProcessed\") \\\n",
    "    .config(\"spark.jars.packages\", f\"{delta_package},{xml_package}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.cores.max\", \"1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "hdfs_path = \"hdfs://spark-test1:9000\"\n",
    "raw = os.path.join(hdfs_path, 'raw', 'transactions')\n",
    "checkpoint = os.path.join(hdfs_path, 'checkpoint', 'processed', 'transactions')\n",
    "dlq = os.path.join(hdfs_path, 'dlq', 'processed', 'transactions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.column import Column, _to_java_column\n",
    "from pyspark.sql.types import _parse_datatype_json_string\n",
    "\n",
    "def ext_from_xml(xml_column, schema, options={}):\n",
    "    java_column = _to_java_column(xml_column.cast('string'))\n",
    "    java_schema = spark._jsparkSession.parseDataType(schema.json())\n",
    "    scala_map = spark._jvm.org.apache.spark.api.python.PythonUtils.toScalaMap(options)\n",
    "    jc = spark._jvm.com.databricks.spark.xml.functions.from_xml(\n",
    "        java_column, java_schema, scala_map)\n",
    "    return Column(jc)\n",
    "\n",
    "def ext_schema_of_xml_df(df, options={}):\n",
    "    assert len(df.columns) == 1\n",
    "\n",
    "    scala_options = spark._jvm.PythonUtils.toScalaMap(options)\n",
    "    java_xml_module = getattr(getattr(\n",
    "        spark._jvm.com.databricks.spark.xml, \"package$\"), \"MODULE$\")\n",
    "    java_schema = java_xml_module.schema_of_xml_df(df._jdf, scala_options)\n",
    "    return _parse_datatype_json_string(java_schema.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Transaction: struct (nullable = true)\n",
      " |    |-- TransactionId: long (nullable = true)\n",
      " |    |-- Amount: float (nullable = true)\n",
      " |    |-- CustomerId: long (nullable = true)\n",
      " |    |-- DateTime: timestamp (nullable = true)\n",
      " |    |-- Location: string (nullable = true)\n",
      " |    |-- Result: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/10 05:13:05 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9c8ecb87-9b11-4652-9b61-dae6bf685e28. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/12/10 05:13:05 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "# Define the schema of your raw layer\n",
    "raw_schema = StructType([\n",
    "    StructField(\"key\", StringType()),\n",
    "    StructField(\"value\", StringType()),\n",
    "    StructField(\"topic\", StringType()),\n",
    "    StructField(\"partition\", StringType()),\n",
    "    StructField(\"offset\", StringType()),\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"timestampType\", StringType()),\n",
    "    StructField(\"_etl_insert_timestamp\", TimestampType()),\n",
    "    StructField(\"_etl_insert_date\", StringType()),\n",
    "    StructField(\"_etl_insert_hour\", IntegerType())\n",
    "])\n",
    "# Read the data from the raw layer using structured streaming\n",
    "raw_df = spark.readStream.format(\"parquet\")\\\n",
    "    .option(\"path\", raw)\\\n",
    "    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .schema(raw_schema) \\\n",
    "    .load()\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"path\", StringType()),\n",
    "    StructField(\"modificationTime\", StringType()),\n",
    "    StructField(\"length\", StringType()),\n",
    "    StructField(\"content\", StringType())\n",
    "])\n",
    "parsed_json_df = raw_df.withColumn(\"json_data\", from_json(col(\"value\"), json_schema))\n",
    "\n",
    "# Extract and decode the base64 content\n",
    "decoded_df = parsed_json_df.withColumn(\"decoded_content\", unbase64(col(\"json_data.content\"))) \\\n",
    "    .withColumn(\"xml_content\", expr(\"CAST(decoded_content AS STRING)\"))\n",
    "\n",
    "#schema_def = ext_schema_of_xml_df(decoded_df.select(\"xml_content\"))\n",
    "#decoded_df = decoded_df.withColumn('test_debug', lit(schema_def).cast('string'))\n",
    "\n",
    "xml_schema = StructType([\n",
    "    StructField(\n",
    "        'Transaction', \n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField('TransactionId', LongType(), True),\n",
    "                StructField('Amount', FloatType(), True),\n",
    "                StructField('CustomerId', LongType(), True),\n",
    "                StructField('DateTime', TimestampType(), True),\n",
    "                StructField('Location', StringType(), True),\n",
    "                StructField('Result', StringType(), True)\n",
    "            ]),\n",
    "            True\n",
    "        ),\n",
    "        True\n",
    "    )\n",
    "])\n",
    "\n",
    "xml_df = decoded_df.withColumn(\n",
    "    \"parsed\",\n",
    "    ext_from_xml(\n",
    "        xml_column = col(\"xml_content\"),\n",
    "        schema=xml_schema,\n",
    "        options={\"mode\": \"FAILFAST\"}\n",
    "    )\n",
    ")\n",
    "# Flatten the DataFrame\n",
    "flattened_df = xml_df.select(\n",
    "    explode(col(\"parsed.Transaction\")).alias(\"Transaction\")\n",
    ")\n",
    "flattened_df.printSchema()\n",
    "flattened_df = flattened_df.select(\n",
    "    col(\"Transaction.Amount\").alias(\"Amount\"),\n",
    "    col(\"Transaction.CustomerId\").alias(\"CustomerId\"),\n",
    "    col(\"Transaction.DateTime\").alias(\"DateTime\"),\n",
    "    upper(trim(col(\"Transaction.Location\"))).alias(\"Location\"),\n",
    "    upper(trim(col(\"Transaction.Result\"))).alias(\"Result\"),\n",
    "    current_date().alias(\"_etl_insert_date\"),\n",
    "    date_format(current_timestamp(), \"HH\").alias(\"_etl_insert_hour\"),\n",
    "    current_timestamp().alias(\"_etl_insert_timestamp\")\n",
    ")\n",
    "\n",
    "# Write the transformed data to the processed layer\n",
    "query = flattened_df.writeStream\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"checkpointLocation\", checkpoint)\\\n",
    "    .trigger(once=True)\\\n",
    "    .format(\"parquet\")\\\n",
    "    .option(\"path\", processed_layer_path)\\\n",
    "    .start()\n",
    "\n",
    "# query = flattened_df \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
