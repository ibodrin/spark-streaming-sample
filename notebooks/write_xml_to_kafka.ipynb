{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/spark/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/spark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/spark/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-94f656d4-56a4-4ce2-b2b6-78496f9e5151;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 356ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-94f656d4-56a4-4ce2-b2b6-78496f9e5151\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/4ms)\n",
      "23/12/10 07:09:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/10 07:09:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/12/10 07:09:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/12/10 07:09:48 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/12/10 07:11:01 WARN FileStreamSource: Listed 4 file(s) in 2006 ms            \n"
     ]
    }
   ],
   "source": [
    "import random, time, threading, os, glob\n",
    "from random import randint\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "host = 'spark-test1'\n",
    "checkpoint = 'hdfs://spark-test1:9000/checkpoint/raw/transactions'\n",
    "xml_directory = 'hdfs://spark-test1:9000/in/transactions'\n",
    "\n",
    "# Set the location of the Delta Lake and Kafka packages\n",
    "kafka_package = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\"  # Replace with the correct Spark version\n",
    "\n",
    "# Initialize Spark Session for Kafka\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"XMLToKafkaProducer\") \\\n",
    "    .master(f\"spark://{host}:7077\") \\\n",
    "    .config(\"spark.jars.packages\", f\"{kafka_package}\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", checkpoint) \\\n",
    "    .config(\"spark.cores.max\", \"1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Kafka Configuration\n",
    "kafka_server = f\"{host}:9092\"\n",
    "topic_name = \"test-topic\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"path\", StringType(), False),\n",
    "    StructField(\"modificationTime\", TimestampType(), False),\n",
    "    StructField(\"length\", LongType(), False),\n",
    "    StructField(\"content\", BinaryType(), True)\n",
    "])\n",
    "\n",
    "# Read stream from directory\n",
    "df = spark.readStream.format(\"binaryFile\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(xml_directory)\n",
    "\n",
    "# Write the stream to Kafka\n",
    "query = df.selectExpr(\"path as key\", \"to_json(struct(*)) AS value\").writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option(\"topic\", topic_name) \\\n",
    "    .start()\n",
    "\n",
    "# query = df \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()\n",
    "    \n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
