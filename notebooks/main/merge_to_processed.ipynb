{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.column import Column, _to_java_column\n",
    "from pyspark.sql.types import _parse_datatype_json_string\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.streaming import StreamingQueryException\n",
    "import traceback, time, os, logging\n",
    "\n",
    "delta_package = \"io.delta:delta-spark_2.12:3.0.0\"\n",
    "xml_package = \"com.databricks:spark-xml_2.12:0.14.0\"\n",
    "spark = SparkSession.builder.appName(\"merge_to_processed\").master('spark://spark-test1:7077') \\\n",
    "    .config(\"spark.jars.packages\", f\"{delta_package},{xml_package}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.cores.max\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "hdfs_path = \"hdfs://spark-test1:9000\"\n",
    "raw = os.path.join(hdfs_path, 'raw', 'transactions')\n",
    "processed = os.path.join(hdfs_path, 'processed', 'transactions')\n",
    "checkpoint = os.path.join(hdfs_path, 'checkpoint', 'processed', 'transactions')\n",
    "dlq_malformed = os.path.join(hdfs_path, 'dlq', 'processed', 'transactions_malformed')\n",
    "dlq_failed_batches = os.path.join(hdfs_path, 'dlq', 'processed', 'transactions_failed_batches')\n",
    "\n",
    "# Mapping XML parsing functions as per https://github.com/databricks/spark-xml?tab=readme-ov-file#pyspark-notes\n",
    "def ext_from_xml(xml_column, schema, options={}):\n",
    "    java_column = _to_java_column(xml_column.cast('string'))\n",
    "    java_schema = spark._jsparkSession.parseDataType(schema.json())\n",
    "    scala_map = spark._jvm.org.apache.spark.api.python.PythonUtils.toScalaMap(options)\n",
    "    jc = spark._jvm.com.databricks.spark.xml.functions.from_xml(\n",
    "        java_column, java_schema, scala_map)\n",
    "    return Column(jc)\n",
    "def ext_schema_of_xml_df(df, options={}):\n",
    "    assert len(df.columns) == 1\n",
    "    scala_options = spark._jvm.PythonUtils.toScalaMap(options)\n",
    "    java_xml_module = getattr(getattr(\n",
    "        spark._jvm.com.databricks.spark.xml, \"package$\"), \"MODULE$\")\n",
    "    java_schema = java_xml_module.schema_of_xml_df(df._jdf, scala_options)\n",
    "    return _parse_datatype_json_string(java_schema.json())\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    if not batch_df.rdd.isEmpty():\n",
    "        try:\n",
    "            files_count = batch_df.count()\n",
    "            \n",
    "            json_schema = StructType([\n",
    "                StructField(\"path\", StringType()),\n",
    "                StructField(\"modificationTime\", StringType()),\n",
    "                StructField(\"length\", StringType()),\n",
    "                StructField(\"content\", StringType())\n",
    "            ])\n",
    "            parsed_json_df = batch_df.withColumn(\"json_data\", from_json(col(\"value\"), json_schema))\n",
    "\n",
    "            # Extract and decode the base64 content\n",
    "            decoded_df = parsed_json_df.withColumn(\"decoded_content\", unbase64(col(\"json_data.content\"))) \\\n",
    "                .withColumn(\"xml_content\", expr(\"CAST(decoded_content AS STRING)\"))\n",
    "\n",
    "            #schema_def = ext_schema_of_xml_df(decoded_df.select(\"xml_content\"))\n",
    "            #decoded_df = decoded_df.withColumn('test_debug', lit(schema_def).cast('string'))\n",
    "\n",
    "            xml_schema = StructType([\n",
    "                StructField(\n",
    "                    'Transaction', \n",
    "                    ArrayType(\n",
    "                        StructType([\n",
    "                            StructField('TransactionId', LongType(), True),\n",
    "                            StructField('Amount', FloatType(), True),\n",
    "                            StructField('CustomerId', LongType(), True),\n",
    "                            StructField('DateTime', TimestampType(), True),\n",
    "                            StructField('Location', StringType(), True),\n",
    "                            StructField('Result', StringType(), True)\n",
    "                        ]),\n",
    "                        True\n",
    "                    ),\n",
    "                    True\n",
    "                )\n",
    "            ])\n",
    "\n",
    "            xml_df = decoded_df.withColumn(\n",
    "                \"parsed\",\n",
    "                ext_from_xml(\n",
    "                    xml_column = col(\"xml_content\"),\n",
    "                    schema=xml_schema,\n",
    "                    options={\n",
    "                        \"mode\": \"PERMISSIVE\",\n",
    "                        \"columnNameOfCorruptRecord\": \"_corrupt_file\"\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if '_corrupt_file' not in xml_df.columns:\n",
    "                xml_df = xml_df.withColumn('_corrupt_file', lit(None))\n",
    "\n",
    "            valid_records = xml_df.filter(col(\"_corrupt_file\").isNull())\n",
    "\n",
    "            if not valid_records.rdd.isEmpty():\n",
    "                windowSpec = Window.partitionBy(\"TransactionId\").orderBy(col(\"_raw_insert_timestamp\").desc())\n",
    "                # Flatten the DataFrame\n",
    "                flattened_df = valid_records.select(\n",
    "                    explode(col(\"parsed.Transaction\")).alias(\"Transaction\"),\n",
    "                    col('_raw_insert_timestamp').alias('_raw_insert_timestamp')\n",
    "                ).select(\n",
    "                    col(\"Transaction.TransactionId\").alias(\"TransactionId\"),\n",
    "                    round(col(\"Transaction.Amount\"), 2).alias(\"Amount\"),\n",
    "                    col(\"Transaction.CustomerId\").alias(\"CustomerId\"),\n",
    "                    col(\"Transaction.DateTime\").alias(\"TransactionDateTime\"),\n",
    "                    to_date(col(\"Transaction.DateTime\")).alias(\"TransactionDate\"),\n",
    "                    upper(trim(col(\"Transaction.Location\"))).alias(\"Location\"),\n",
    "                    upper(trim(col(\"Transaction.Result\"))).alias(\"Status\"),\n",
    "                    current_timestamp().alias(\"_processed_insert_timestamp\"),\n",
    "                    col('_raw_insert_timestamp').alias('_raw_insert_timestamp'),\n",
    "                    lit(batch_id).alias('_batch_id')\n",
    "                ).withColumn(\"row_rank\", row_number().over(windowSpec)) \\\n",
    "                    .filter(col(\"row_rank\") == 1) \\\n",
    "                    .drop(\"row_rank\")\n",
    "\n",
    "                # Check for the existence of the Delta table\n",
    "                if DeltaTable.isDeltaTable(spark, processed):\n",
    "                    # If the table exists, create a DeltaTable instance for it\n",
    "                    delta_table = DeltaTable.forPath(spark, processed)\n",
    "                    # Perform the merge operation\n",
    "                    delta_table.alias(\"target\").merge(\n",
    "                        flattened_df.alias(\"source\"),\n",
    "                        \"target.TransactionId = source.TransactionId\"\n",
    "                    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "                else:\n",
    "                    # If the Delta table does not exist, create one from the batch DataFrame\n",
    "                    flattened_df.write.format(\"delta\").partitionBy(\"TransactionDate\").save(processed)\n",
    "\n",
    "            malformed_records = xml_df.filter(col(\"_corrupt_file\").isNotNull())\n",
    "            if not malformed_records.rdd.isEmpty():\n",
    "                malformed_records \\\n",
    "                    .withColumn(\"_batch_id\", lit(batch_id)) \\\n",
    "                    .withColumn(\"_error_insert_date\", current_date()) \\\n",
    "                    .withColumn(\"_error_insert_hour\", date_format(current_timestamp(), \"HH\")) \\\n",
    "                    .withColumn(\"_error_insert_timestamp\", current_timestamp()) \\\n",
    "                    .write.mode(\"append\") \\\n",
    "                    .partitionBy(\"_error_insert_date\", \"_error_insert_hour\") \\\n",
    "                    .parquet(dlq_malformed)\n",
    "            \n",
    "            print(f\"Processed {files_count} files in batch {batch_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            traceback_text = traceback.format_exc()\n",
    "\n",
    "            # Add error details to the batch DataFrame\n",
    "            failed_batch_df = batch_df.withColumn(\"error_message\", lit(error_message)) \\\n",
    "                .withColumn(\"_traceback\", lit(traceback_text)) \\\n",
    "                .withColumn(\"_batch_id\", lit(batch_id)) \\\n",
    "                .withColumn(\"_error_insert_date\", current_date()) \\\n",
    "                .withColumn(\"_error_insert_hour\", date_format(current_timestamp(), \"HH\")) \\\n",
    "                .withColumn(\"_error_insert_timestamp\", current_timestamp())\n",
    "\n",
    "            # Save the failed batch with error details to the DLQ\n",
    "            failed_batch_df.write.mode(\"append\") \\\n",
    "                .partitionBy(\"_error_insert_date\", \"_error_insert_hour\") \\\n",
    "                .parquet(dlq_failed_batches)\n",
    "\n",
    "    else:\n",
    "        print(\"Empty batch\")\n",
    "\n",
    "\n",
    "def stream(spark):\n",
    "    raw_schema = StructType([\n",
    "        StructField(\"key\", StringType()),\n",
    "        StructField(\"value\", StringType()),\n",
    "        StructField(\"topic\", StringType()),\n",
    "        StructField(\"partition\", StringType()),\n",
    "        StructField(\"offset\", StringType()),\n",
    "        StructField(\"timestamp\", StringType()),\n",
    "        StructField(\"timestampType\", StringType()),\n",
    "        StructField(\"_raw_insert_timestamp\", TimestampType()),\n",
    "        StructField(\"_raw_insert_date\", StringType()),\n",
    "        StructField(\"_raw_insert_hour\", IntegerType())\n",
    "    ])\n",
    "\n",
    "    df = spark.readStream.format(\"parquet\")\\\n",
    "        .option(\"path\", raw)\\\n",
    "        .schema(raw_schema) \\\n",
    "        .load()\n",
    "    \n",
    "    return df.writeStream \\\n",
    "        .foreachBatch(process_batch) \\\n",
    "        .trigger(availableNow=True) \\\n",
    "        .option(\"checkpointLocation\", checkpoint) \\\n",
    "        .start()\n",
    "\n",
    "    # return df.writeStream \\\n",
    "    #     .outputMode(\"append\") \\\n",
    "    #     .format(\"console\") \\\n",
    "    #     .start()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "retries = 3\n",
    "retry_num = 0\n",
    "succeeded = None\n",
    "while True:\n",
    "    if retry_num == retries - 1:\n",
    "        break\n",
    "    try:\n",
    "        stream(spark).awaitTermination()\n",
    "        succeeded = True\n",
    "        logger.info(\"Batch processing completed\")\n",
    "        break\n",
    "    except StreamingQueryException as e:\n",
    "        retry_num += 1\n",
    "        # Log the error message\n",
    "        print(f\"Streaming exception:\\n{traceback.format_exc()}\")\n",
    "        print(\"Restarting query after 10 seconds...\")\n",
    "        time.sleep(10)  # Sleep for 10 seconds before restarting the query\n",
    "    except Exception as e:\n",
    "        retry_num += 1\n",
    "        print(f\"Non-streaming exception:\\n{traceback.format_exc()}\")\n",
    "        print(f\"Restarting query after 10 seconds...\")        \n",
    "        time.sleep(10)\n",
    "\n",
    "if not succeeded:\n",
    "    logger.error(\"Batch processing failed\")\n",
    "    pass # TODO send alert that job did not succeed after retries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
