# spark-streaming-sample

## Purpose
This repository provides sample Spark Structured Streaming application with the use-case of Transaction details sent as XML documents to Apache Kafka and ingested to raw/bronze and processed/silver layers in HDFS.

## Application Logic
1. XML documents are generated by [`scripts/misc/generate_xml.py`](scripts/misc/generate_xml.py) and stored in HDFS
2. Streaming job [`notebooks/main/write_to_kafka.ipynb`](notebooks/main/write_to_kafka.ipynb) is reading XML files and writing them to Kafka topic
3. Streaming job [`notebooks/main/write_to_raw.ipynb`](notebooks/main/write_to_raw.ipynb) is reading files from the Kafka topic and writing them as-is along with source filesystem metadata to Parquet table partitioned by ingestion date and hour columns in `Raw` layer on HDFS
4. Scheduled streaming job [`notebooks/main/merge_to_processed.ipynb`](notebooks/main/merge_to_processed.ipynb) with `trigger=availableNow` (ensures job completion upon processing data increment available as of the job start time) is reading unprocessed (since the last execution) data in `Raw` layer and performs XML parsing/flattening with necessary data enrichment followed by the upsert into Delta table in `Processed` layer based on regular intervals (e.g. hourly or on-demand)

## Instructions
1. Save `docker-compose.yml` on your workstation
2. Run `docker compose -f "docker-compose.yml" up -d --build` and `docker-compose -f "docker-compose.yml" logs -f spark-master` to start the demo container and print the startup log in the console
3. All services and spark jobs will start up automatically (allow 5 minutes to start - once the Jupyter log shows up in the output the startup is complete)
4. Open `http://localhost:8888` and use the pre-defined [`notebooks/test/test.ipynb`](notebooks/test/test.ipynb) notebook to view the ingested transaction details in `Raw` and `Processed` layers with Jupyter

## Data Format
* Input - see comment with sample XML in [`scripts/misc/generate_xml.py`](scripts/misc/generate_xml.py)
* Output - see `Processed` layer sample in [`notebooks/test/test.ipynb`](notebooks/test/test.ipynb)

## Notes
* `spark-submit` scripts can be found in [`scripts/shell`](scripts/shell)
* [`scripts/shell/start_merge_to_processed.sh`](scripts/shell/start_merge_to_processed.sh) automatically exists if the previous scheduled `merge_to_processed` job instance is still running (this is possible if the number of available batches as of the job startup time is very large and can't be processed within the required scheduling interval)
* Spark UI is available at `http://localhost:8080` and `http://localhost:4040`
* Distribution versions used (under `/opt`):
  * `spark-3.5.0`
  * `kafka_2.13-3.6.1`
  * `hadoop-3.3.6`

## Referenced Documentation
1. https://spark.apache.org/docs/latest/
2. https://kafka.apache.org/documentation/
3. https://hadoop.apache.org/docs/stable/
4. https://github.com/databricks/spark-xml
